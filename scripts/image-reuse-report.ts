/**
 * Image reuse report:
 * - Reads data/normalized-menu.json (generated by ingest-revel-to-sanity.ts).
 * - Collects all imageUrl entries and finds duplicates by URL and by basename.
 * - Checks for local files with matching basenames in:
 *     - data/revel/.../images/
 *     - public/images/
 *     - images/ (if it exists)
 * - Emits a report to stdout and optionally writes JSON via --out <file>.
 *
 * Run:
 *   npx ts-node scripts/image-reuse-report.ts
 *   npx ts-node scripts/image-reuse-report.ts --out data/image-reuse-report.json
 */

import fs from "node:fs";
import path from "node:path";

type NormalizedPayload = {
  stores: Array<{
    storeSlug: string;
    storeId?: number;
    items: Array<{
      name: string;
      image?: string | null;
    }>;
  }>;
};

type Args = {
  out?: string | null;
};

function parseArgs(argv: string[]): Args {
  const outIdx = argv.findIndex((a) => a === "--out");
  return { out: outIdx >= 0 && argv[outIdx + 1] ? argv[outIdx + 1] : null };
}

function loadNormalized(filePath = path.resolve("data/normalized-menu.json")): NormalizedPayload {
  if (!fs.existsSync(filePath)) {
    throw new Error(`Normalized file not found: ${filePath}. Run ingest-revel-to-sanity.ts --out first.`);
  }
  return JSON.parse(fs.readFileSync(filePath, "utf-8")) as NormalizedPayload;
}

function scanLocalFiles(): Record<string, string[]> {
  const roots = [
    path.resolve("data/revel"),
    path.resolve("public/images"),
    path.resolve("images")
  ];
  const hits: Record<string, string[]> = {};

  for (const root of roots) {
    if (!fs.existsSync(root)) continue;
    const stat = fs.statSync(root);
    if (!stat.isDirectory()) continue;
    walk(root, (file) => {
      const base = path.basename(file);
      hits[base] = hits[base] || [];
      hits[base].push(file);
    });
  }
  return hits;
}

function walk(dir: string, onFile: (file: string) => void) {
  for (const entry of fs.readdirSync(dir)) {
    const full = path.join(dir, entry);
    const stat = fs.statSync(full);
    if (stat.isDirectory()) {
      walk(full, onFile);
    } else {
      onFile(full);
    }
  }
}

function slugify(input: string): string {
  return input
    .toLowerCase()
    .normalize("NFKD")
    .replace(/[^a-z0-9]+/g, "-")
    .replace(/-+/g, "-")
    .replace(/(^-|-$)/g, "");
}

function main() {
  const args = parseArgs(process.argv.slice(2));
  const normalized = loadNormalized();

  const urlMap: Record<string, { count: number; stores: Set<string> }> = {};
  const baseMap: Record<string, { urls: Set<string>; stores: Set<string> }> = {};
  const slugMap: Record<string, { urls: Set<string>; stores: Set<string> }> = {};

  for (const store of normalized.stores) {
    for (const item of store.items) {
      const url = item.image;
      if (!url) continue;
      urlMap[url] = urlMap[url] || { count: 0, stores: new Set() };
      urlMap[url].count += 1;
      urlMap[url].stores.add(store.storeSlug);
      const base = path.basename(new URL(url).pathname);
      baseMap[base] = baseMap[base] || { urls: new Set(), stores: new Set() };
      baseMap[base].urls.add(url);
      baseMap[base].stores.add(store.storeSlug);
      const slug = slugify(item.name);
      slugMap[slug] = slugMap[slug] || { urls: new Set(), stores: new Set() };
      slugMap[slug].urls.add(url);
      slugMap[slug].stores.add(store.storeSlug);
    }
  }

  const dupUrls = Object.entries(urlMap).filter(([, v]) => v.stores.size > 1);
  const dupBases = Object.entries(baseMap).filter(([, v]) => v.urls.size > 1 || v.stores.size > 1);

  const localFiles = scanLocalFiles();
  const reuseMatches: Array<{ basename: string; localPaths: string[]; urls: string[]; stores: string[] }> = [];
  const slugMatches: Array<{ slug: string; localPaths: string[]; urls: string[]; stores: string[] }> = [];
  for (const [base, info] of Object.entries(baseMap)) {
    if (localFiles[base]) {
      reuseMatches.push({
        basename: base,
        localPaths: localFiles[base],
        urls: Array.from(info.urls),
        stores: Array.from(info.stores)
      });
    }
  }
  // slug-based: if any local filename contains the slug
  for (const [slug, info] of Object.entries(slugMap)) {
    const hits: string[] = [];
    for (const [base, paths] of Object.entries(localFiles)) {
      if (base.toLowerCase().includes(slug)) {
        hits.push(...paths);
      }
    }
    if (hits.length) {
      slugMatches.push({
        slug,
        localPaths: Array.from(new Set(hits)),
        urls: Array.from(info.urls),
        stores: Array.from(info.stores)
      });
    }
  }

  const report = {
    totalImages: Object.keys(urlMap).length,
    duplicatedByUrl: dupUrls.map(([url, info]) => ({
      url,
      appearances: info.count,
      stores: Array.from(info.stores)
    })),
    duplicatedByBasename: dupBases.map(([base, info]) => ({
      basename: base,
      urls: Array.from(info.urls),
      stores: Array.from(info.stores)
    })),
    reuseMatches,
    slugMatches
  };

  console.log("== Image Reuse Report ==");
  console.log(`Unique image URLs: ${report.totalImages}`);
  console.log(`Duplicates by URL (shared across stores): ${report.duplicatedByUrl.length}`);
  console.log(`Duplicates by basename: ${report.duplicatedByBasename.length}`);
  console.log(`Local reuse matches (basename found locally): ${report.reuseMatches.length}`);
  console.log(`Local slug matches (local filename contains slug): ${report.slugMatches.length}`);

  if (args.out) {
    fs.mkdirSync(path.dirname(args.out), { recursive: true });
    fs.writeFileSync(args.out, JSON.stringify(report, null, 2), "utf-8");
    console.log(`Saved detailed report to ${path.resolve(args.out)}`);
  } else {
    console.log("\nPass --out <file> to save full JSON report.");
  }
}

main();
